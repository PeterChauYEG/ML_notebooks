{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Flower Classifer\n",
    "We are going to classify of flowers by species. The dataset which we will use for this toy DNN is the [Iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set). This is a multivariate dataset from 1936 - Ronald Fisher. There are 50 samples for each of the 3 species of Iris ((Iris setosa, Iris virginica and Iris versicolor). There are 4 features for each sample measured in cm:\n",
    "- length of sepals\n",
    "- width of sepals\n",
    "- length of petals\n",
    "- width of petals\n",
    "\n",
    "Tensorflow provides a [reference](https://www.tensorflow.org/get_started/estimator) for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependancies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure TF Logging\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "model_dir_root = '/tmp/iris'\n",
    "model_dir = os.path.join(model_dir_root, \"deep_\" + str(int(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Params\n",
    "batch_size = 10\n",
    "num_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataset ():\n",
    "    # If the training and test sets aren't stored locally, download them\n",
    "    if not os.path.exists(IRIS_TRAINING):\n",
    "        with urllib.request.urlopen(IRIS_TRAINING_URL) as url: \n",
    "            raw = url.read()\n",
    "        with open(IRIS_TRAINING, \"wb\") as f:\n",
    "            f.write(raw)\n",
    "\n",
    "    if not os.path.exists(IRIS_TEST):\n",
    "        with urllib.request.urlopen(IRIS_TEST_URL) as url: \n",
    "            raw = url.read()\n",
    "        with open(IRIS_TEST, \"wb\") as f:\n",
    "            f.write(raw)\n",
    "            \n",
    "    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "        filename=IRIS_TRAINING,\n",
    "        target_dtype=np.int,\n",
    "        features_dtype=np.float32)\n",
    "    \n",
    "    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "        filename=IRIS_TEST,\n",
    "        target_dtype=np.int,\n",
    "        features_dtype=np.float32)\n",
    "    \n",
    "    return training_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_data(dataset, example, all=False):\n",
    "    if all == True:\n",
    "        # log the training dataset\n",
    "        print(dataset)\n",
    "\n",
    "    # log 1 example and 1 answer\n",
    "    print(\"X: {}\".format(dataset[0][example]))\n",
    "    print(\"Y: {}\".format(dataset[1][example]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build input function\n",
    "def generate_input_fn(dataset, batch_size=batch_size):\n",
    "    def _input_fn():\n",
    "        X = tf.constant(dataset[0])\n",
    "        Y = tf.constant(dataset[1], dtype=tf.int32)\n",
    "        \n",
    "        X_batch, Y_batch = tf.train.shuffle_batch(\n",
    "            [X,Y],\n",
    "            batch_size=batch_size,\n",
    "            capacity=8*batch_size,\n",
    "            min_after_dequeue=4*batch_size,\n",
    "            enqueue_many=True\n",
    "        )\n",
    "        \n",
    "        return {'features': X_batch}, Y_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build classifier\n",
    "def define_and_run_dnn_classifier(num_steps, logdir, lr=.1, batch_size=batch_size):\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column('features', dimension=4)]\n",
    "\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        n_classes=3,\n",
    "        hidden_units=[10,10,10],\n",
    "        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=lr),\n",
    "        model_dir=logdir\n",
    "    )\n",
    "    \n",
    "    # Train classifer\n",
    "    classifier.train(\n",
    "        input_fn=generate_input_fn(\n",
    "            training_set,\n",
    "            batch_size=batch_size\n",
    "        ),\n",
    "        steps=num_steps\n",
    "    )\n",
    "    \n",
    "    print(\"Finished running the deep training\")\n",
    "    print(\"evaluating DNN classifier accuracy\")\n",
    "    \n",
    "    # Test classifer\n",
    "    accuracy_score = classifier.evaluate(\n",
    "        input_fn=generate_input_fn(\n",
    "            test_set,\n",
    "            batch_size=batch_size\n",
    "        ),\n",
    "        steps=100\n",
    "    )['accuracy']\n",
    "    \n",
    "    print(\"DNN classifier accuracy: {0:f}\".format(accuracy_score))\n",
    "    \n",
    "    # Make a prediction\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=generate_input_fn(\n",
    "            test_set,\n",
    "            batch_size=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # make a prediction\n",
    "    print(\"DNN classifier prediction: \")\n",
    "    for i in range(10):\n",
    "        prediction = predictions.__next__()['probabilities']\n",
    "        print(\"\\nPrediction for example {0}: {1}\".format(i, np.argmax(prediction)))\n",
    "        log_data(test_set, i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run classifer\n",
    "training_set, test_set = loadDataset()\n",
    "\n",
    "# log an example\n",
    "log_data(training_set, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Running DNN Classifier\")\n",
    "define_and_run_dnn_classifier(\n",
    "    num_steps,\n",
    "    model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir=/tmp/iris"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
