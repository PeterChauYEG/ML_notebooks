{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Birth Weight\n",
    "\n",
    "## [Data](http://www.statlab.uni-heidelberg.de/data/linmod/birthweight.html)\n",
    "There is a birth weight dataset with 189 entires. These rows include 1 label as to weither the baby has a low birth weight, and 9 features.\n",
    "\n",
    "## Goal\n",
    "We want to classify an example with 9 features into low birth weight or not. Low birthweight is < 2.5kg\n",
    "\n",
    "## Methodology\n",
    "We are going to use a DNN classifier to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependancies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure TF Logging\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "model_dir_root = '/tmp/low_birthweight'\n",
    "model_dir = os.path.join(model_dir_root, \"deep_\" + str(int(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Params\n",
    "batch_size = 8\n",
    "num_steps = 1250\n",
    "Dataset = collections.namedtuple('Dataset', ['data', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "birth_weight_file = 'birth_weight.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_dataset ():\n",
    "    # download data and create data file if file does not exist in current directory\n",
    "    if not os.path.exists(birth_weight_file):\n",
    "        birthdata_url = 'https://github.com/nfmcclure/tensorflow_cookbook/raw/master/01_Introduction/07_Working_with_Data_Sources/birthweight_data/birthweight.dat'\n",
    "        birth_file = requests.get(birthdata_url)\n",
    "        birth_data = birth_file.text.split('\\r\\n')\n",
    "        birth_header = birth_data[1].split('\\t')\n",
    "        birth_data = [[float(x) for x in y.split('\\t') if len(x)>=1] for y in birth_data[1:] if len(y)>=1]\n",
    "        with open(birth_weight_file, \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(birth_data)\n",
    "            f.close()\n",
    "\n",
    "    # read birth weight data into memory\n",
    "    birth_data = []\n",
    "    with open(birth_weight_file, newline='') as csvfile:\n",
    "         csv_reader = csv.reader(csvfile)\n",
    "         birth_header = next(csv_reader)\n",
    "         for row in csv_reader:\n",
    "             birth_data.append(row)\n",
    "\n",
    "    birth_data = [[float(x) for x in row] for row in birth_data]\n",
    "\n",
    "    # Pull out target variable\n",
    "    y_vals = np.array([x[0] for x in birth_data])\n",
    "    # Pull out predictor variables (not id, not target, and not birthweight)\n",
    "    x_vals = np.array([x[1:9] for x in birth_data])\n",
    "\n",
    "    # set for reproducible results\n",
    "    seed = 99\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    # Split data into train/test = 80%/20%\n",
    "    train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\n",
    "    test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
    "    x_vals_train = x_vals[train_indices]\n",
    "    x_vals_test = x_vals[test_indices]\n",
    "    y_vals_train = y_vals[train_indices]\n",
    "    y_vals_test = y_vals[test_indices]\n",
    "    \n",
    "    # Create training_set Database Object\n",
    "    train_target = np.array(y_vals_train, dtype=np.int)\n",
    "    train_data = np.array(x_vals_train)\n",
    "    training_set = Dataset(data=train_data, target=train_target)\n",
    "    \n",
    "    # Create test_set Database Object\n",
    "    test_target = np.array(y_vals_test, dtype=np.int)\n",
    "    test_data = np.array(x_vals_test)\n",
    "    test_set = Dataset(data=test_data, target=test_target)    \n",
    "    \n",
    "    return training_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_data(dataset, example, all=False):\n",
    "    if all == True:\n",
    "        # log the training dataset\n",
    "        print(dataset)\n",
    "\n",
    "    # log 1 example and 1 answer\n",
    "    print(\"X: {}\".format(dataset[0][example]))\n",
    "    print(\"Y: {}\".format(dataset[1][example]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build input function\n",
    "def generate_input_fn(dataset, batch_size=batch_size):\n",
    "    def _input_fn():\n",
    "        X = tf.constant(dataset[0])\n",
    "        Y = tf.constant(dataset[1], dtype=tf.int32)\n",
    "        \n",
    "        X_batch, Y_batch = tf.train.shuffle_batch(\n",
    "            [X,Y],\n",
    "            batch_size=batch_size,\n",
    "            capacity=3*batch_size,\n",
    "            min_after_dequeue=2*batch_size,\n",
    "            enqueue_many=True\n",
    "        )\n",
    "        \n",
    "        return {'features': X_batch}, Y_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build classifier\n",
    "def define_and_run_dnn_classifier(num_steps, logdir, lr=.1, batch_size=batch_size):\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column('features', dimension=8)]\n",
    "\n",
    "#     linear classifier    \n",
    "#     classifier = tf.estimator.LinearClassifier(\n",
    "#         feature_columns=feature_columns,\n",
    "#         model_dir=logdir,\n",
    "#         n_classes=2,\n",
    "#         optimizer='Ftrl',\n",
    "#     )\n",
    "    \n",
    "#     DNN Classifier\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        n_classes=2,\n",
    "        hidden_units=[10, 10],\n",
    "        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=lr),\n",
    "        model_dir=logdir\n",
    "    )\n",
    "    \n",
    "    # Train classifer\n",
    "    classifier.train(\n",
    "        input_fn=generate_input_fn(\n",
    "            training_set,\n",
    "            batch_size=batch_size\n",
    "        ),\n",
    "        steps=num_steps\n",
    "    )\n",
    "    \n",
    "    print(\"Finished running the deep training\")\n",
    "    print(\"evaluating DNN classifier accuracy\")\n",
    "    \n",
    "    # Test classifer\n",
    "    accuracy_score = classifier.evaluate(\n",
    "        input_fn=generate_input_fn(\n",
    "            test_set,\n",
    "            batch_size=batch_size\n",
    "        ),\n",
    "        steps=100\n",
    "    )['accuracy']\n",
    "    \n",
    "    print(\"DNN classifier accuracy: {0:f}\".format(accuracy_score))\n",
    "    \n",
    "    # Make a prediction\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=generate_input_fn(\n",
    "            test_set,\n",
    "            batch_size=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # make a prediction\n",
    "    print(\"DNN classifier prediction: \")\n",
    "    for i in range(10):\n",
    "        prediction = predictions.__next__()['probabilities']\n",
    "        print(\"\\nPrediction for example {0}: {1}\".format(i, np.argmax(prediction)))\n",
    "        log_data(test_set, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_set, test_set = construct_dataset()\n",
    "\n",
    "# log an example\n",
    "log_data(training_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Running DNN Classifier\")\n",
    "define_and_run_dnn_classifier(\n",
    "    num_steps,\n",
    "    model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir=/tmp/low_birthweight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
