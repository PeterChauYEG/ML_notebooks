{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing Data\n",
    "We are going to solve this data by predicting the Median value of owner-occupied homes in 1000s. I'm going to use a Neural Network'd regressor.\n",
    "\n",
    "## Data\n",
    "samples: 506\n",
    "features: real, positive\n",
    "Total size = [506, 14]\n",
    "\n",
    "We need to pull out the label\n",
    "x: [506, 13]\n",
    "y: [506, 1]\n",
    "\n",
    "data = { [x, y] }\n",
    "\n",
    "Then we need to split the data\n",
    "80%/20%\n",
    "\n",
    "Training set: 80%\n",
    "training_set = { x_train, y_train }\n",
    "\n",
    "Test set: 20%\n",
    "test_set = { x_test, y_test }\n",
    "\n",
    "## Implementation\n",
    "1. Get the data\n",
    "2. Preprocess the data\n",
    "3. Setup model\n",
    "4. Setup input pipelines\n",
    "5. Train the model\n",
    "6. Test the model\n",
    "7. Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependancies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "## New stuff\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# instanitate dataset object\n",
    "Dataset = collections.namedtuple('Dataset', ['data', 'target'])\n",
    "    \n",
    "# TF Logging\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Directories\n",
    "model_dir_root = '/tmp/housing'\n",
    "# model_dir = os.path.join(model_dir_root, \"linear_\" + str(int(time.time())))\n",
    "model_dir = os.path.join(model_dir_root, \"deep_\" + str(int(time.time())))\n",
    "data_file = 'boston_housing.csv'\n",
    "\n",
    "# hyper parameters\n",
    "batch_size = 50\n",
    "num_steps = 10000\n",
    "lr=0.1\n",
    "rs=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data ():\n",
    "#     raw_data = '[0,0,0,0,0,0,0,0,0,0,0,0,0,0]'\n",
    "    raw_data = datasets.load_boston()\n",
    "    \n",
    "    return raw_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_data (data):\n",
    "    print('Description: {}\\n'.format(data.DESCR))\n",
    "    print('Features: {}\\n'.format(data.feature_names))\n",
    "    print('Example 0: {}\\n'.format(data.data[0]))\n",
    "    print('Target 0: {}\\n'.format(data.target[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset (data, test_size):\n",
    "    # set x and y\n",
    "    x, y = data.data, data.target\n",
    "    \n",
    "    # split data\n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "        x, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Create the training set\n",
    "    train_set = create_dataset(x_train, y_train)\n",
    "    \n",
    "    # Create the test set\n",
    "    test_set = create_dataset(x_test, y_test)\n",
    "    \n",
    "    return test_set, train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset (x, y):\n",
    "    # convert to np\n",
    "    target = np.array(y, dtype=np.int)\n",
    "    data = np.array(x)\n",
    "    \n",
    "    # merge into Dataset object\n",
    "    new_dataset = Dataset(data=data, target=target)\n",
    "    \n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_dataset ():\n",
    "    # get the data\n",
    "    raw_data = get_data()\n",
    "    \n",
    "    # explore the data\n",
    "#     explore_data(raw_data)\n",
    "    \n",
    "    # split the dataset \n",
    "    # 80%/20%\n",
    "    test_set, training_set = split_dataset(raw_data, .2)\n",
    "\n",
    "    return test_set, training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_data(dataset, example, all=False):\n",
    "    if all == True:\n",
    "        # log the training dataset\n",
    "        print(dataset)\n",
    "\n",
    "    # log 1 example and 1 answer\n",
    "    print(\"X: {}\".format(dataset[0][example]))\n",
    "    print(\"Y: {}\".format(dataset[1][example]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_input_fn(dataset, batch_size=batch_size):\n",
    "    def _input_fn():\n",
    "        X = tf.constant(dataset[0])\n",
    "        Y = tf.constant(dataset[1], dtype=tf.float32)\n",
    "        \n",
    "        X_batch, Y_batch = tf.train.shuffle_batch(\n",
    "            [X,Y],\n",
    "            batch_size=batch_size,\n",
    "            capacity=8*batch_size,\n",
    "            min_after_dequeue=4*batch_size,\n",
    "            enqueue_many=True\n",
    "        )\n",
    "        \n",
    "        return {'features': X_batch}, Y_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_and_run_dnn_regressor(num_steps, logdir, lr=.1, batch_size=batch_size):\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column('features', dimension=13)]\n",
    "    \n",
    "    estimator = tf.estimator.DNNRegressor(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[10,10,10],\n",
    "        model_dir=logdir,\n",
    "        optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "            learning_rate=lr,\n",
    "            l1_regularization_strength=rs\n",
    "        ),\n",
    "        activation_fn=tf.nn.relu)\n",
    "    \n",
    "    # Train the estimator\n",
    "    print(\"Starting deep training\")\n",
    "    estimator.train(\n",
    "        input_fn=generate_input_fn(\n",
    "            training_set,\n",
    "            batch_size=batch_size\n",
    "        ),\n",
    "        steps=num_steps)\n",
    "    print(\"Finished deep training\")\n",
    "    print(\"evaluating DNN regressor accuracy\")\n",
    "    \n",
    "    # Test classifer\n",
    "    accuracy_score = estimator.evaluate(\n",
    "        input_fn=generate_input_fn(\n",
    "            test_set,\n",
    "            batch_size=batch_size\n",
    "        ),\n",
    "        steps=10\n",
    "    )['average_loss']\n",
    "    \n",
    "    print(\"DNN regressor average loss: {0}\".format(accuracy_score))\n",
    "    \n",
    "    # Make a prediction\n",
    "    predictions = estimator.predict(\n",
    "        input_fn=generate_input_fn(\n",
    "            test_set,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # make a prediction\n",
    "    print(\"DNN regressor prediction: \")\n",
    "    for i in range(10):\n",
    "        prediction = predictions.__next__()['predictions'][0]\n",
    "        print(\"\\nPrediction for example {0}: {1}\".format(i, prediction))\n",
    "        log_data(training_set, i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Main Program\n",
    "test_set, training_set = construct_dataset()\n",
    "\n",
    "define_and_run_dnn_regressor(\n",
    "    num_steps,\n",
    "    model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir=/tmp/housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
